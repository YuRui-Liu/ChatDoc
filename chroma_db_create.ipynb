{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 知识库的构建\n",
    "### 文档解析-->分块-->embedding-->向量数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import chromadb\n",
    "from model import RagEmbedding, RagLLM, QwenLLM\n",
    "from tools.doc_parse import chunk, read_and_process_excel, logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载文档集\n",
    "pdf_files = ['./data/zhidu/zhidu_employee.pdf',\n",
    "            './data/zhidu/zhidu_travel.pdf']\n",
    "\n",
    "excel_files = ['./data/zhidu/zhidu_detail.xlsx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化递归文本分割对象\n",
    "r_spliter = RecursiveCharacterTextSplitter(chunk_size=128,\n",
    "                                          chunk_overlap=30,\n",
    "                                          separators=[\"\\n\\n\",\n",
    "                                                      \"\\n\",\n",
    "                                                      \".\",\n",
    "                                                      \"\\uff0e\",  # Fullwidth full stop\n",
    "                                                       \"\\u3002\",  # Ideographic full stop\n",
    "                                                      \",\",\n",
    "                                                      \"\\uff0c\",  # Fullwidth comma\n",
    "                                                      \"\\u3001\",  # Ideographic comma\n",
    "                                                     ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解析并切割pdf内容\n",
    "doc_data = []\n",
    "for pdf_file_name in pdf_files:\n",
    "    res = chunk(pdf_file_name, callback=logger)\n",
    "    for data in res:\n",
    "        content = data[\"content_with_weight\"]\n",
    "        if '<table>' not in content and len(content) > 200:\n",
    "            # 超过200字符，使用递归文本对象再次分块\n",
    "            doc_data = doc_data + r_spliter.split_text(content)\n",
    "        else:\n",
    "            doc_data.append(content)\n",
    "\n",
    "for i in doc_data:\n",
    "    print(len(i), \"=\"*10,  i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'excel_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 处理excel\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m excel_file_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mexcel_files\u001b[49m:\n\u001b[0;32m      3\u001b[0m     data \u001b[38;5;241m=\u001b[39m read_and_process_excel(excel_file_name)\n\u001b[0;32m      4\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data[\u001b[38;5;241m8\u001b[39m:], columns\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m7\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'excel_files' is not defined"
     ]
    }
   ],
   "source": [
    "# 处理excel\n",
    "for excel_file_name in excel_files:\n",
    "    data = read_and_process_excel(excel_file_name)\n",
    "    df = pd.DataFrame(data[8:], columns=data[7])\n",
    "    data_excel = df.drop(columns=df.columns[11:17]) # 去掉空列\n",
    "    content = data_excel.to_markdown(index=False).replace(' ', \"\")\n",
    "    doc_data.append(f\"### 以下是中央和国家机关工作人员赴地方差旅住宿费标准明细表： \\n\\n {content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Desktop\\LLM资料\\Proj\\ChatDoc_RagLangChain\\ChatDoc1.0\\model.py:97: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embedding = HuggingFaceEmbeddings(model_name=model_path,\n"
     ]
    }
   ],
   "source": [
    "# 封装成document，以支持向量化\n",
    "from langchain_core.documents import Document\n",
    "import uuid\n",
    "documents = []\n",
    "document_ids = []\n",
    "for idx, chunk in enumerate(doc_data):\n",
    "    is_table = 0\n",
    "    if \"table\" in chunk:\n",
    "        is_table = 1\n",
    "    if idx == len(doc_data) - 1:\n",
    "        is_table = 1\n",
    "    doc_id = str(uuid.uuid4())\n",
    "    document = Document(\n",
    "        page_content=chunk,\n",
    "        metadata={\"type\": \"ori\", \"is_table\": is_table})\n",
    "    documents.append(document)\n",
    "    document_ids.append(doc_id)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 向量化处理\n",
    "embedding_cls = RagEmbedding(model_path=\"./data/llm_app/embedding_models/bge-m3//\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于pickle化\n",
    "doc_txts = dict(zip(document_ids, documents))\n",
    "\n",
    "import pickle\n",
    "with open('./data/zhidu/zhidu_db.pickl', 'wb') as file:\n",
    "    pickle.dump(doc_txts, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chromadb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 与Chroma向量库建立连接\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m chroma_client \u001b[38;5;241m=\u001b[39m \u001b[43mchromadb\u001b[49m\u001b[38;5;241m.\u001b[39mHttpClient(host\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocalhost\u001b[39m\u001b[38;5;124m\"\u001b[39m, port\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8000\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 创建向量库对象\u001b[39;00m\n\u001b[0;32m      5\u001b[0m embedding_db \u001b[38;5;241m=\u001b[39m Chroma\u001b[38;5;241m.\u001b[39mfrom_documents(documents,\n\u001b[0;32m      6\u001b[0m                                      embedding_cls\u001b[38;5;241m.\u001b[39mget_embedding_fun(),\n\u001b[0;32m      7\u001b[0m                                      client\u001b[38;5;241m=\u001b[39mchroma_client,\n\u001b[0;32m      8\u001b[0m                                      collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzhidu_db\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m                                      )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chromadb' is not defined"
     ]
    }
   ],
   "source": [
    "# 与Chroma向量库建立连接\n",
    "chroma_client = chromadb.HttpClient(host=\"localhost\", port=8000)\n",
    "\n",
    "# 创建向量库对象\n",
    "embedding_db = Chroma.from_documents(documents,\n",
    "                                     embedding_cls.get_embedding_fun(),\n",
    "                                     client=chroma_client,\n",
    "                                     collection_name=\"zhidu_db\",\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试向量搜索\n",
    "query = '迟到有什么规定？'\n",
    "\n",
    "# 找到相似度最高的前k个文本块\n",
    "related_docs = embedding_db.similarity_search(query, k=2)\n",
    "\n",
    "\n",
    "print(\"related_docs:\" ,related_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
